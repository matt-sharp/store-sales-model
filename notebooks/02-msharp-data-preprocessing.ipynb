{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "%load_ext nb_black\n",
    "# Load the \"autoreload\" extension so that code can change\n",
    "%load_ext autoreload\n",
    "# always reload modules so that as you change code in src, it gets loaded\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import shap  # package used to calculate Shap values\n",
    "from matplotlib import pyplot as plt\n",
    "from rfpimp import *\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import _VectorizerMixin\n",
    "from sklearn.feature_selection._base import SelectorMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import (\n",
    "    FunctionTransformer,\n",
    "    OneHotEncoder,\n",
    "    OrdinalEncoder,\n",
    ")\n",
    "from sklearn.preprocessing._encoders import _BaseEncoder\n",
    "\n",
    "\n",
    "# from category_encoders import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE\n",
    "This notebook code has been written in a mostly procedural format.  This is so that I can demonstrate my thought process and decision making in a clear and logical way.  This code would need some re-factoring once I'm happy with my final solution, before thinking about getting it to a production ready format if required.\n",
    "\n",
    "I have used 'Black' code reformatter to ensure the style is PEP 8 compliant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# from category_encoders import OrdinalEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "This step of the process is crucial for obtaining an accurate model.  It is also the most time consuming.  I've spent my time creating an end-to-end solution and therefore haven't dived as deeply into all of the necessary steps to produce the best model.\n",
    "Below I have listed most of the techniques I would normally try:\n",
    " - Imputation\n",
    "- Drop records or columns with a high proportion of missing values\n",
    "- Replace missing values with mean/median/mode (numeric) or mode (categorical)\n",
    " - One-hot Encoding\n",
    "- Map categorical feature to N-1 binary columns\n",
    "- Try to not create a sparse dataframe if feature contains many categories\n",
    " - Outliers\n",
    "- Remove outliers entirely\n",
    "- Capping the outliers using percentile cut-offs\n",
    "- Binning the values\n",
    " - Simplify relationships to either binary flags or straight lines\n",
    " - Transformations\n",
    "- Apply different transformations based on the shape of the distribution\n",
    "- Square root, logarithm, inverse and reflect the distribution when it is negatively skewed\n",
    "- Try to straighten the curve of a line to extract a stronger relationship\n",
    " - Mean Enconding for categorical features\n",
    "- For categorical features, encode each level as the mean of the response\n",
    "- Achieves many objectives including scaling, outliers, NULL values and interpretability\n",
    "- Binning\n",
    "- For continuous features try quartiles, quintiles, deciles\n",
    " - Binning with Decision Trees\n",
    " - Polynomial Features, e.g. xsquared, xcubed\n",
    " - Feature Interactions, e.g. x1x2\n",
    " - Derived Features using domain knowledge\n",
    "\n",
    "NOTE:\n",
    "Random Forest is a tree-based model that uses a partitioning algorithm and does not require feature scaling.  The decision on where to split/partition the data will not change with scaling, i.e. the tree only sees ranks in the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access store variables\n",
    "%store -r df\n",
    "%store -r predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can do some of the pre-processing on the training and prediction data at the same time\n",
    "df_combined = pd.concat([df, predict], axis=0, sort=False)\n",
    "print(df_combined.shape)\n",
    "df_combined.head(-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert object to category dtypes\n",
    "df_combined[\"transport_availability\"] = df_combined[\"transport_availability\"].astype(\n",
    "    \"category\"\n",
    ")\n",
    "df_combined[\"county\"] = df_combined[\"county\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the ordinal values to integers\n",
    "mapping_dict = {\n",
    "    \"transport_availability\": {\n",
    "        \"All transport options\": 5,\n",
    "        \"Many transport options\": 4,\n",
    "        \"Average transport options\": 3,\n",
    "        \"Few transport options\": 2,\n",
    "        \"No transport options\": 1,\n",
    "    }\n",
    "}\n",
    "df_combined.replace(mapping_dict, inplace=True)\n",
    "df_combined.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode new_store feature\n",
    "df_combined = pd.get_dummies(df_combined, columns=[\"new_store\"])\n",
    "\n",
    "# check distribution of feature\n",
    "print(df_combined[\"new_store_yes\"].value_counts())\n",
    "\n",
    "df_combined.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode county values with numeric labels\n",
    "df_combined[\"county\"] = df_combined[\"county\"].cat.codes\n",
    "df_combined.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define list of features before further transformed features are added\n",
    "# orig_features = df_combined.drop(columns=[\"normalised_sales\", \"data\"]).columns\n",
    "orig_features = df_combined.drop(columns=[\"normalised_sales\", \"data\"]).columns\n",
    "orig_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply transformations to some of the features\n",
    "\n",
    "# apply log transformation adding one, to avoid negative values\n",
    "df_combined[\"log_proportion_newbuilds\"] = df_combined[\"proportion_newbuilds\"].apply(\n",
    "    np.log1p\n",
    ")\n",
    "df_combined[\"log_public_transport_dist\"] = df_combined[\"public_transport_dist\"].apply(\n",
    "    np.log1p\n",
    ")\n",
    "# reflect the distribution first before applying log, since distribution is negatively skewed\n",
    "df_combined[\"log_competitor_density\"] = (\n",
    "    (df_combined[\"competitor_density\"].max() + 1) -\n",
    "    df_combined[\"competitor_density\"]\n",
    ").apply(np.log1p)\n",
    "\n",
    "# apply inverse transformation\n",
    "df_combined[\"inv_commercial_property\"] = df_combined[\"commercial_property\"].apply(\n",
    "    np.reciprocal\n",
    ")\n",
    "# reflect the distribution first before applying inverse, since distribution is negatively skewed\n",
    "df_combined[\"inv_competitor_density\"] = (\n",
    "    (df_combined[\"competitor_density\"].max() + 1) -\n",
    "    df_combined[\"competitor_density\"]\n",
    ").apply(np.reciprocal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Dataset\n",
    "- We should re-use the parameters for the transformations applied to the training set for use on the test set to avoid information leakage\n",
    "- Any predictions made should also use the same parameters\n",
    "- If the dataset is sufficiently large we shouldn't expect to see much difference between the train and test parameters since we assume all samples have been drawn from the same distribution\n",
    "- Information leakage can cause optimism bias in the model evaluation\n",
    "- We should not use the response in the test set for anything except comparing to our predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "# split data for prediction\n",
    "predict = df_combined.loc[df_combined[\"data\"] == \"predict\"]\n",
    "predict = predict.drop(columns=[\"normalised_sales\", \"data\"])\n",
    "print(\"Predict Features Shape:\", predict.shape)\n",
    "\n",
    "# use a seed value throughout notebook to ensure results are reproducible\n",
    "seed = 42\n",
    "\n",
    "# split the target value\n",
    "X = df_combined.loc[df_combined[\"data\"] == \"train\"].drop(\n",
    "    columns=[\"normalised_sales\", \"data\"]\n",
    ")\n",
    "y = df_combined.loc[df_combined[\"data\"] == \"train\"][\"normalised_sales\"]\n",
    "\n",
    "# split the data into training and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=seed\n",
    ")\n",
    "\n",
    "print(\"Training Features Shape:\", X_train.shape)\n",
    "print(\"Training Labels Shape:\", y_train.shape)\n",
    "print(\"Validation Features Shape:\", X_test.shape)\n",
    "print(\"Validation Labels Shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn estimators assume all values are numerical and that all have and hold meaning.\n",
    "\n",
    "More advanced techniques that could be tried include:\n",
    "- To avoid discarding entire rows and/or columns containing missing values we can impute missing values.\n",
    "- Multivariate Feature Imputation -\n",
    " - Model each feature with missing values as a function of other features and use that estimate for imputation\n",
    " - It does so in an iterated round-robin fashion: at each step, a feature column is designated as output y and the other feature columns are treated as inputs X.\n",
    " - A regressor is fit on (X, y) for known y. Then, the regressor is used to predict the missing values of y.\n",
    "- KNN Feature Imputation -\n",
    " - A euclidean distance metric that supports missing values, nan_euclidean_distances, is used to find the nearest neighbors.\n",
    " - Each missing feature is imputed using values from n_neighbors nearest neighbors that have a value for the feature.\n",
    " - The feature of the neighbors are averaged uniformly or weighted by distance to each neighbor.\n",
    " - If a sample has more than one feature missing, then the neighbors for that sample can be different depending on the particular feature being imputed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_missing_values(df):\n",
    "    df[\"commercial_property\"].fillna(\n",
    "        (X_train[\"commercial_property\"].median()), inplace=True\n",
    "    )\n",
    "    df[\"inv_commercial_property\"].fillna(\n",
    "        (X_train[\"inv_commercial_property\"].median()), inplace=True\n",
    "    )\n",
    "    #     df[\"log_crime_rate\"].fillna(\n",
    "    #         (X_train[\"log_commercial_property\"].median()), inplace=True\n",
    "    #     )\n",
    "    #     df[\"log_proportion_flats\"].fillna(\n",
    "    #         (X_train[\"log_proportion_flats\"].median()), inplace=True\n",
    "    #     )\n",
    "    #     df[\"inv_crime_rate\"].fillna(\n",
    "    #         (X_train[\"inv_commercial_property\"].median()), inplace=True\n",
    "    #     )\n",
    "    #     df[\"cap_commercial_property\"].fillna(\n",
    "    #         (X_train[\"cap_commercial_property\"].median()), inplace=True\n",
    "    #     )\n",
    "    #     df[\"cap_proportion_flats\"].fillna(\n",
    "    #         (X_train[\"cap_proportion_flats\"].median()), inplace=True\n",
    "    #     )\n",
    "    df[\"school_proximity\"].fillna(\n",
    "        (X_train[\"school_proximity\"].median()), inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "replace_missing_values(X_train)\n",
    "replace_missing_values(X_test)\n",
    "replace_missing_values(predict)\n",
    "\n",
    "# check for missing values again\n",
    "print(\"X_train missing values:\", X_train.isnull().values.any())\n",
    "print(\"X_test missing values:\", X_test.isnull().values.any())\n",
    "print(\"Predict missing values:\", predict.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create histogram showing distribution of commercial_property before capping outliers\n",
    "hist1 = X_train[\"commercial_property\"].plot(\n",
    "    kind=\"hist\", bins=20, figsize=(10, 5), title=\"commercial_property: Not Capped\"\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# cap outliers at the 1% and 99% percentile level\n",
    "\n",
    "\n",
    "def cap_outliers(df, cap1, cap2):\n",
    "\n",
    "    commercial_property_percentiles = (\n",
    "        X_train[\"commercial_property\"].quantile([cap1, cap2]).values\n",
    "    )\n",
    "    proportion_flats_percentiles = (\n",
    "        X_train[\"proportion_flats\"].quantile([cap1, cap2]).values\n",
    "    )\n",
    "\n",
    "    np.clip(\n",
    "        df[\"commercial_property\"],\n",
    "        commercial_property_percentiles[0],\n",
    "        commercial_property_percentiles[1],\n",
    "        inplace=True,\n",
    "    )\n",
    "    np.clip(\n",
    "        df[\"proportion_flats\"],\n",
    "        proportion_flats_percentiles[0],\n",
    "        proportion_flats_percentiles[1],\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    # df['capped_commercial_property'] = np.clip(df['commercial_property'], commercial_property_percentiles[0], commercial_property_percentiles[1])\n",
    "    # df['capped_proportion_flats'] = np.clip(df['proportion_flats'], proportion_flats_percentiles[0], proportion_flats_percentiles[1])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "cap_outliers(X_train, 0.01, 0.99)\n",
    "cap_outliers(X_test, 0.01, 0.99)\n",
    "cap_outliers(predict, 0.01, 0.99)\n",
    "\n",
    "# create histogram showing distribution of commercial_property after capping outliers\n",
    "hist2 = X_train[\"commercial_property\"].plot(\n",
    "    kind=\"hist\", bins=20, figsize=(10, 5), title=\"commercial_property: Capped\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define labels for bins\n",
    "labels = [\"low\", \"high\"]\n",
    "\n",
    "# bin using quartile cut-off to ensure even split of samples\n",
    "X_train[\"binned_commercial_property\"], bins_commercial_property = pd.qcut(\n",
    "    X_train[\"commercial_property\"], q=2, retbins=True, labels=labels\n",
    ")\n",
    "# bin using cut since distribution of this feature is so imbalanced\n",
    "X_train[\"binned_proportion_flats\"], bins_proportion_flats = pd.cut(\n",
    "    X_train[\"proportion_flats\"], 2, retbins=True, labels=labels\n",
    ")\n",
    "\n",
    "# apply bin cut-offs from training data to test set\n",
    "X_test[\"binned_commercial_property\"] = pd.cut(\n",
    "    X_test[\"commercial_property\"], bins=bins_commercial_property, labels=labels\n",
    ")\n",
    "X_test[\"binned_proportion_flats\"] = pd.cut(\n",
    "    X_test[\"proportion_flats\"], bins=bins_proportion_flats, labels=labels\n",
    ")\n",
    "\n",
    "# apply bin cut-offs from training data to predict set\n",
    "predict[\"binned_commercial_property\"] = pd.cut(\n",
    "    predict[\"commercial_property\"], bins=bins_commercial_property, labels=labels\n",
    ")\n",
    "predict[\"binned_proportion_flats\"] = pd.cut(\n",
    "    predict[\"proportion_flats\"], bins=bins_proportion_flats, labels=labels\n",
    ")\n",
    "\n",
    "# one-hot encode new binned features\n",
    "X_train = pd.get_dummies(\n",
    "    X_train, columns=[\"binned_commercial_property\", \"binned_proportion_flats\"]\n",
    ")\n",
    "X_test = pd.get_dummies(\n",
    "    X_test, columns=[\"binned_commercial_property\", \"binned_proportion_flats\"]\n",
    ")\n",
    "predict = pd.get_dummies(\n",
    "    predict, columns=[\"binned_commercial_property\", \"binned_proportion_flats\"]\n",
    ")\n",
    "\n",
    "X_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines\n",
    "To simplify the process of applying transformations it's best to use a pipeline. They have several key benefits:\n",
    " - They make the workflow much easier to read and understand.\n",
    " - They enforce the implementation and order of steps in the project.\n",
    " - These in turn make the work much more reproducible.\n",
    "\n",
    "If we receive further prediction data in future or this model needs to be put into production then it's much more desirable to simply pass the data through a pipeline.\n",
    "\n",
    "NOTE - Due to time constraints I haven't fully constructed a pipeline with all of the necessary components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this implementation of OrdinalEncoder is required to allow for unknown categories in unseen data\n",
    "# the handle_categories option has not been rleased in sklearn yet\n",
    "# Do not use from sklearn.preprocessing import _BaseEncoder, it is protected class!\n",
    "\n",
    "\n",
    "class new_OrdinalEncoder(_BaseEncoder):\n",
    "    def __init__(self, cat_index=\"all\"):\n",
    "        self.dicts = {}\n",
    "        # cate_index is the categorical feature index list\n",
    "        self.cat_index = cat_index\n",
    "\n",
    "    def fit(self, df, *y):\n",
    "        if self.cat_index == \"all\":\n",
    "            self.cat_index = list(range(df.shape[1]))\n",
    "        for feat in self.cat_index:\n",
    "            dic = np.unique(df.iloc[:, feat])\n",
    "            dic = dict([(i, index) for index, i in enumerate(dic)])\n",
    "            self.dicts[feat] = dic\n",
    "\n",
    "    def fit_transform(self, df, *y):\n",
    "        if self.cat_index == \"all\":\n",
    "            self.cat_index = list(range(df.shape[1]))\n",
    "        df_output = df.copy()\n",
    "        for feat in self.cat_index:\n",
    "            dic = np.unique(df.iloc[:, feat])\n",
    "            dic = dict([(i, index) for index, i in enumerate(dic)])\n",
    "            self.dicts[feat] = dic\n",
    "            df_output.iloc[:, feat] = df.iloc[:, feat].apply(lambda x: dic[x])\n",
    "        return df_output\n",
    "\n",
    "    def transform(self, df):\n",
    "        df_output = df.copy()\n",
    "        for feat in self.cat_index:\n",
    "            dic = self.dicts[feat]\n",
    "            df_output.iloc[:, feat] = df.iloc[:, feat].apply(\n",
    "                self.unknown_value, args=(dic,)\n",
    "            )\n",
    "        return df_output\n",
    "\n",
    "    def unknown_value(\n",
    "        self, value, dic\n",
    "    ):  # It will set up a new interger for unknown values!\n",
    "        try:\n",
    "            return dic[value]\n",
    "        except:\n",
    "            return len(dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OrdinalEncoder\n",
    " - If a new value that never appears in the fit step needs to be converted in the transform step, OrdinalEncoder will raise an error.\n",
    " - Such unknown value will cause a problem when the testing set contains an unknown value of the training set.\n",
    " - In order to avoid unknown value in the testing set, we have to fit the entire data set for the OrdinalEncoder, which means we need to fit the OrdinalEncoder before splitting the dataset into training and testing.\n",
    " - Even if we can fix unknown value in this way, it still does not work when new samples with unknown value fed into the model.\n",
    " - Due to the above I have used a custom implementation of OrdinalEncoder that handles missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = train.copy()\n",
    "\n",
    "# define the ordering for the categories used in the mapping of the ordinal feature\n",
    "categories = [\n",
    "    \"All transport options\",\n",
    "    \"Many transport options\",\n",
    "    \"Average transport options\",\n",
    "    \"Few transport options\",\n",
    "    \"No transport options\",\n",
    "]\n",
    "\n",
    "# replace missing values with median\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"median\"))])\n",
    "\n",
    "# apply logarithm transform\n",
    "log_transformer = Pipeline(steps=[(\"log\", FunctionTransformer(np.log1p))])\n",
    "\n",
    "# apply inverse transform\n",
    "inv_transformer = Pipeline(steps=[(\"inv\", FunctionTransformer(np.reciprocal))])\n",
    "\n",
    "# one-hot encode categorical feature\n",
    "# if an unknown category is encountered during transform, the resulting ohe columns for this feature will all be zeros\n",
    "categorical_transformer1 = Pipeline(\n",
    "    steps=[(\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))]\n",
    ")\n",
    "\n",
    "# transform categorical feature to numeric with ordinal mapping\n",
    "categorical_transformer2 = Pipeline(\n",
    "    steps=[(\"ordinal1\", OrdinalEncoder(categories=[categories]))]\n",
    ")\n",
    "\n",
    "# transform categorical feature to numeric with lexical (default method) ordering\n",
    "# use a different implementation of OrdinalEncoder above to allow for unknown categories not seen in training data\n",
    "categorical_transformer3 = Pipeline(steps=[(\"ordinal2\", new_OrdinalEncoder())])\n",
    "\n",
    "numeric_features = (\n",
    "    train.drop(columns=[\"normalised_sales\"])\n",
    "    .select_dtypes(include=[\"int64\", \"float64\"])\n",
    "    .columns\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features)  # ,\n",
    "        #        ('log', log_transformer, ['proportion_newbuilds','public_transport_dist'])\n",
    "        ,\n",
    "        #        ('inv', inv_transformer, ['commercial_property'])\n",
    "        #        ,\n",
    "        (\"cat1\", categorical_transformer1, [\"new_store\"]),\n",
    "        (\"cat2\", categorical_transformer2, [\"transport_availability\"]),\n",
    "        (\"cat3\", categorical_transformer3, [\"county\"]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that transformations are being applied correctly\n",
    "def get_feature_out(estimator, feature_in):\n",
    "    if hasattr(estimator, \"get_feature_names\"):\n",
    "        if isinstance(estimator, _VectorizerMixin):\n",
    "            # handling all vectorizers\n",
    "            return [f\"vec_{f}\" for f in estimator.get_feature_names()]\n",
    "        else:\n",
    "            return estimator.get_feature_names(feature_in)\n",
    "    elif isinstance(estimator, SelectorMixin):\n",
    "        return np.array(feature_in)[estimator.get_support()]\n",
    "    else:\n",
    "        return feature_in\n",
    "\n",
    "\n",
    "def get_ct_feature_names(ct):\n",
    "    # handles all estimators, pipelines inside ColumnTransfomer\n",
    "    # doesn't work when remainder =='passthrough'\n",
    "    # which requires the input column names.\n",
    "    output_features = []\n",
    "\n",
    "    for name, estimator, features in ct.transformers_:\n",
    "        if name != \"remainder\":\n",
    "            if isinstance(estimator, Pipeline):\n",
    "                current_features = features\n",
    "                for step in estimator:\n",
    "                    current_features = get_feature_out(step, current_features)\n",
    "                features_out = current_features\n",
    "            else:\n",
    "                features_out = get_feature_out(estimator, features)\n",
    "            output_features.extend(features_out)\n",
    "        elif estimator == \"passthrough\":\n",
    "            output_features.extend(ct._feature_names_in[features])\n",
    "\n",
    "    return output_features\n",
    "\n",
    "\n",
    "transformed_data = preprocessor.fit_transform(train)\n",
    "\n",
    "pd.DataFrame(transformed_data, columns=get_ct_feature_names(preprocessor))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
